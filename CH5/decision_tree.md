# decision tree

## 特征

* 分类与回归
* if-then 的集合
* 递归
* 次最优（sub-optimal）

## 前置知识

### 特征选择

特征选择在于选取对训练数据具有分类能力的特征，若选择某一特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上去掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益（information gain）或信息增益比。

**熵和条件熵**

熵（entropy）：表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大。

设 $X$ 是一个取有限个值（记为 $n$）的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i,\quad i=1,2,\cdots,n\\
x\in\{x_1,x_2,\cdots,x_n\}
$$
则随机变量 $X$ 的熵的定义为
$$
H(X)=-\sum_{i=1}^{n}{p_i\log{p_i}},\quad 0\leq H(X)\leq \log n
$$
上式中，若 $p_i=0$，则定义 $0\log 0=0$。若上式中的对数以 2 为底或以 e 为底（自然对数），则熵的单位分别为比特（bit）或纳特（nat）。易知熵只依赖于 $X$ 的分布，与 $X$ 的取值无关，故也可记作 $H(p)$。

条件熵（conditional entropy） $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。
$$
\begin{aligned}
H(Y|X)&=\sum_{i=1}^{n}P(X=x_i)H(Y|X=x_i)\\
&=-\sum_{x}{p(x)H(Y|X=x)}\\
&=-\sum_{x}{p(x)\sum_{y}{p(y|x)\log{p(y|x)}}}\\
&=-\sum_{x}\sum_{y}p(x,y)\log{p(y|x)}\\
&=-\sum_{x}\sum_{y}p(x,y)\log{\frac{p(x,y)}{p(x)}}\\
&=-\sum_{x}\sum_{y}p(x,y)\log{p(x,y)}+\sum_{x}\sum_{y}p(x,y)\log{p(x)}\\
&=-H(X,Y)+\sum_{x}p(x)\log{p(x)}\\
&=H(X,Y)-H(X)\\
\end{aligned}
$$
熵和条件熵若由极大似然估计得到，又分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。

**信息增益**

信息增益：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与给定特征 $A$ 的条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即：
$$
g(D,A)=H(D)-H(D|A)
$$
该值也称为互信息（mutual information），表示由于特征 $A$ 而使得对数据集 $D$ 的分类的不确定减少的程度。易知信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法：对训练数据集 $D$，计算其每个特征的信息增益，选择信息增益最大的（部分）特征。

**信息增益比**

以信息增益作为选取特征的指标存在偏向于选择（在相同分布下）取值较多的特征的问题。使用信息增益比（information gain ratio）可以校正这一问题。

定义：特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
H_A(D)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
$$

## 模型

给定数据集
$$
D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中，$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$ 为输入实例（特征向量），$n$ 为特征个数，$y_i\in\{1,2,\cdots,K\},i=1,2,\cdots,N$ 为类标记，$N$ 为样本容量。决策树的将构建一个决策树模型，使其能够对实例正确分类。

## 策略

设树 $T$ 的叶节点个数为 $|T|$，$t$ 是某一叶节点，该叶节点有 $N_t$ 个样本点；第 $t$ 个叶节点中第 $k$ 类的样本有 $N_{tk}$ 个，$t=1,2,\cdots,N_{t};k=1,2,\cdots,K$；$H_{t}(T)$ 为叶节点 $t$ 上的经验熵，$\alpha \geq 0$ 为参数，则决策树学习的损失函数可以定义为：
$$
C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha |T|
$$
其中经验熵为：
$$
H_t(T)=-\sum_k \frac{N_{tk}}{N_t}\log{\frac{N_{tk}}{N_t}}
$$
记$\sum_{t=1}^{|T|}N_tH_t(T)$ 为 $C(T)$，有：
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log\frac{N_{tk}}{N_t}
$$
这时有：
$$
C_{\alpha}(T)=C(T)+\alpha |T|
$$
其中 $C(T)$ 表示模型对训练数据的预测误差，$|T|$ 表示模型复杂度；较大的 $\alpha$ 促使选择简单、对训练数据的拟合度低的模型，较小的 $\alpha$ 促使选择复杂、对训练数据的拟合度高的模型。

## 算法

### 1.信息增益

输入：训练集 $D$ 和特征 A；

输出：特征 $A$ 对数据集 $D$ 的信息增益 $g(D,A)$。

算法：

1. 计算数据集 $D$ 的经验熵 $H(D)$：
   $$
   \begin{aligned}
   H(D)&=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}\\
   &=-\sum_{k=1}^{K}p(y_k)\log_2{p(y_k)}\\
   y\in\{&y_1,y_2,\cdots.y_K\},\quad \sum_{k=1}^{K}{|C_k|}=|D|
   \end{aligned}
   $$

2. 计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$：
   $$
   \begin{aligned}
   H(D|A)&=\sum_{a}{p(a)H(D|A=a)}\\
   &=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D|A=a)\\
   &=\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{d}{p(d|a)\log{p(d|a)}}\\
   &=\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
   \end{aligned}
   $$
   注：数据集 $D$ 可以根据标签 y 的取值（离散）分为 K 类，每一类为子集 $D_k,1\leq k\leq K$，该子集样本数量为 $|D_i|$；同时可以根据特征 A 的取值（离散）分为 n 类，每一类为子集 $D_i,1\leq i\leq n$，该子集样本数量为 $|D_i|$。

3. 计算信息增益：
   $$
   g(D,A)=H(D)-H(D|A)
   $$

### 2.决策树生成：ID3 算法

输入：训练集 $D$，特征集 $A$ 阈值 $\epsilon$；

输出：决策树 $T$。

算法：

1. 如果 $D$ 属于同一类 $C_k$，$T$ 为单节点树，类 $C_k$ 作为该节点的类标记，返回 $T$；
2. 如果 $A$ 是空集，置 $T$ 为单节点树，实例数最多的类作为该节点类标记，返回 $T$；
3. 否则，计算 $A$ 中各特征对 $D$ 的信息增益 $g$，选择信息增益最大的特征 $A_g$；
4. 如果 $A_g$ 的信息增益小于 $\epsilon$，$T$ 为单节点树，$D$ 中实例数最大的类 $C_k$ 作为类标记，返回 $T$；
5. 否则根据 $A_g$ 的每一可能值 $a_i$，将 $D$ 划分为若干非空子集 $D_i$，将 $|D_i|$ 最大的类作为标记，构建子节点；由节点及其子节点构成树 $T$，返回 $T$；
6. 对每一子节点，即 $D_i$ 训练集，以 $A-A_g$ 为特征集，递归调用步骤 $1\sim5$，得到  $T_i$，返回 $T_i$；

由于 ID3 算法只有树的生成，所以容易过拟合。

### 3.决策树生成： C4.5算法

采用信息增益比来选择特征。

输入：训练集 $D$，特征集 $A$ 阈值 $\epsilon$；

输出：决策树 $T$。

算法：

1. 如果 $D$ 属于同一类 $C_k$，$T$ 为单节点树，类 $C_k$ 作为该节点的类标记，返回 $T$；
2. 如果 $A$ 是空集，置 $T$ 为单节点树，实例数最多的类作为该节点类标记，返回 $T$；
3. 否则，计算 $A$ 中各特征对 $D$ 的信息增益 $g_R$，选择信息增益最大的特征 $A_g$；
4. 如果 $A_g$ 的信息增益小于 $\epsilon$，$T$ 为单节点树，$D$ 中实例数最大的类 $C_k$ 作为类标记，返回 $T$；
5. 否则根据 $A_g$ 的每一可能值 $a_i$，将 $D$ 划分为若干非空子集 $D_i$，将 $|D_i|$ 最大的类作为标记，构建子节点；由节点及其子节点构成树 $T$，返回 $T$；
6. 对每一子节点，即 $D_i$ 训练集，以 $A-A_g$ 为特征集，递归调用步骤 $1\sim5$，得到  $T_i$，返回 $T_i$；

### 4.树的剪枝算法



输入：生成算法产生的整个树 $T$，参数 $\alpha$；

输出：修剪后的子树 $T_{\alpha}$。

算法：

1. 